# AI-102:

https://esi.learnondemand.net/
0EF7AE23B6C94298

## Day2:

### Containers:

- On Prem (install the runtime)
- ACI (only for POC not for prod) need container runtime (eg. Docker) by default provisions only one container
- Arc -enabled
- AKS -> language service
  user -> containers -> FQDN
  container images
- [NER(Named Entity Recognition)](https://learn.microsoft.com/en-us/azure/ai-services/language-service/named-entity-recognition/how-to/use-containers)
- billing -> endpoints
- EULA (end user licence agreement) -> accept
- API key -> access key

disconnected environment supported by containers (contact azure support, purchase commitment tier)

[Azure AI containers](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-container-support)

Container instances -> create container service
Nvidia V100 tensor go
Azure AI services always Linux

Containers used so that client data remains on premises

### Security:

1. CMK(Customer Managed Key)/MMK(Microsoft Managed Key):
   Powered by (key vault) generate keys in key vault and import in your data.
2. Support of TLS 1.2: by default.(secure socket) client should be compatible with TLS 1.2. Old clients may not be.
3. Customer Lockbox: when we create ticket for training failing due to corrupted data.
4. Virtual Network:
5. Access Keys: -> service principal

```powershell
az ad sp create-for-rbac --name <name> --scope <scope>
```

## Labs:

### Lab 1:

#### ai services

- create a azure ai multi-service account
- in the created service we have http endpoint, keys, location
- headers:

```json
{
  "Content-type": "application/json",
  "Ocp-Apim-Subscription-key": "<key>"
}
```

- data:

```json
{
  "documents": [{ "id": 1, "text": "<text>" }]
}
```

- /text/analytics/v3.1/languages?
- AzureKeyCredential
- TextAnalyticsClient
- .env:
  ```dotenv
  AI_SERVICE_ENDPOINT=YOUR_AI_SERVICES_ENDPOINT
  AI_SERVICE_KEY=YOUR_AI_SERVICES_KEY
  ```
- rest-client.py:

  ```python
  from dotenv import load_dotenv
  import os
  import http.client, base64, json, urllib
  from urllib import request, parse, error
  def main():
  	global ai_endpoint
  	global ai_key
  	try:
  		# Get Configuration Settings
  		load_dotenv()
  		ai_endpoint = os.getenv('AI_SERVICE_ENDPOINT')
  		ai_key = os.getenv('AI_SERVICE_KEY')
  		# Get user input (until they enter "quit")
  		userText =''
  		while userText.lower() != 'quit':
  			userText = input('Enter some text ("quit" to stop)\n')
  			if userText.lower() != 'quit':
  				GetLanguage(userText)
  	except Exception as ex:
  		print(ex)

  def GetLanguage(text):
  	try:
  	# Construct the JSON request body (a collection of documents, each with an ID and text)
  	jsonBody = {
  		"documents":[ {"id": 1, "text": text}]}
  		# Let's take a look at the JSON we'll send to the service
  		print(json.dumps(jsonBody, indent=2))
  		# Make an HTTP request to the REST interface
  		uri = ai_endpoint.rstrip('/').replace('https://', '')
  		conn = http.client.HTTPSConnection(uri)
  		# Add the authentication key to the request header
  		headers = {'Content-Type': 'application/json','Ocp-Apim-Subscription-Key': ai_key}
  		# Use the Text Analytics language API
  		conn.request("POST", "/text/analytics/v3.1/languages?", str(jsonBody).encode('utf-8'), headers)
  		# Send the request
  		response = conn.getresponse()
  		data = response.read().decode("UTF-8")
  		# If the call was successful, get the response
  		if response.status == 200:
  			# Display the JSON response in full (just so we can see it)
  			results = json.loads(data)
  			print(json.dumps(results, indent=2))
  			# Extract the detected language name for each document
  			for document in results["documents"]:
  				print("\nLanguage:", document["detectedLanguage"]["name"])
  		else:
  			# Something went wrong, write the whole response
  			print(data)
  		conn.close()
  	except Exception as ex:
  		print(ex)

  if __name__ == "__main__":
  	main()
  ```

- sdk-client.py:

  ```python
  from dotenv import load_dotenv
  import osfrom azure.core.credentials
  import AzureKeyCredentialfrom azure.ai.textanalytics
  import TextAnalyticsClient
  def main():
  	global ai_endpoint
  	global ai_key
  	try:
  		# Get Configuration Settings
  		load_dotenv()
  		ai_endpoint = os.getenv('AI_SERVICE_ENDPOINT')
  		ai_key = os.getenv('AI_SERVICE_KEY')
  		# Get user input (until they enter "quit")
  		userText =''
  		while userText.lower() != 'quit':
  			userText = input('\nEnter some text ("quit" to stop)\n')
  			if userText.lower() != 'quit':
  				language = GetLanguage(userText)
  				print('Language:', language)
  	except Exception as ex:
  		print(ex)

  def GetLanguage(text):
  	# Create client using endpoint and key
  	credential = AzureKeyCredential(ai_key)
  	client = TextAnalyticsClient(endpoint=ai_endpoint, credential=credential)
  	# Call the service to get the detected language
  	detectedLanguage = client.detect_language(documents = [text])[0]
  	return detectedLanguage.primary_language.name

  if __name__ == "__main__":
  	main()
  ```

#### service security:

- keyvault-client.py
- .env:
  ```dotenv
  AI_SERVICE_ENDPOINT=your_ai_services_endpoint
  KEY_VAULT=your_key_vault_name
  TENANT_ID=your_service_principal_tenant_id
  APP_ID=your_service_principal_app_id
  APP_PASSWORD=your_service_principal_password
  ```
- keyvault-client.py

  ```python
  from dotenv import load_dotenv
  import os
  from azure.ai.textanalytics
  import TextAnalyticsClientfrom azure.core.credentials import AzureKeyCredential
  from azure.keyvault.secrets import SecretClient
  from azure.identity import ClientSecretCredential
  def main():
  	global ai_endpoint
  	global cog_key
  	try:
  		# Get Configuration Settings
  		load_dotenv()
  		ai_endpoint = os.getenv('AI_SERVICE_ENDPOINT')
  		key_vault_name = os.getenv('KEY_VAULT')
  		app_tenant = os.getenv('TENANT_ID')
  		app_id = os.getenv('APP_ID')
  		app_password = os.getenv('APP_PASSWORD')
  		# Get Azure AI services key from keyvault using the service principal credentials
  		key_vault_uri = f"https://{key_vault_name}.vault.azure.net/"
  		credential = ClientSecretCredential(app_tenant, app_id, app_password)
  		keyvault_client = SecretClient(key_vault_uri, credential)
  		secret_key = keyvault_client.get_secret("AI-Services-Key")
  		cog_key = secret_key.value
  		# Get user input (until they enter "quit")
  		userText =''
  		while userText.lower() != 'quit':
  			userText = input('\nEnter some text ("quit" to stop)\n')
  			if userText.lower() != 'quit':
  				language = GetLanguage(userText)
  				print('Language:', language)
  	except Exception as ex:
  		print(ex)

  def GetLanguage(text):
  	# Create client using endpoint and key
  	credential = AzureKeyCredential(cog_key)
  	client = TextAnalyticsClient(endpoint=ai_endpoint, credential=credential)
  	# Call the service to get the detected language
  	detectedLanguage = client.detect_language(documents = [text])[0]
  	return detectedLanguage.primary_language.name

  if __name__ == "__main__":
  	main()
  ```

- rest-test.cmd

  ```cmd
  curl -X POST "<your-endpoint>/language/:analyze-text?api-version=2023-04-01" -H "Content-Type: application/json" -H "Ocp-Apim-Subscription-Key: <your-key>" --data-ascii "{'analysisInput':{'documents':[{'id':1,'text':'hello'}]}, 'kind': 'LanguageDetection'}"
  ```

- rest-test.sh
  ```sh
  curl -X POST "<your-endpoint>/text/analytics/v3.1/languages?'" -H "Content-Type: application/json" -H "Ocp-Apim-Subscription-Key: <your-key>" --data-ascii "{'documents':[{'id':1,'text':'hello'}]}"
  ```

### Lab 2(Analyze images with azure ai vision):

- https://github.com/MicrosoftLearning/mslearn-ai-vision/tree/main/Instructions/Exercises

#### Analyze image:

##### .env

```dotenv
AI_SERVICE_ENDPOINT=https://azure-ai-vision-demo-mm.openai.azure.com/
AI_SERVICE_KEY=9590wHeILzgCapXkJ2CEmWiA5AFyWFoSrS6MZpOu55hmsMDLXIZHJQQJ99AKACYeBjFXJ3w3AAAAACOGPgEO
```

##### image-analysis.py

```cmd
pip install azure-ai-vision-imageanalysis==1.0.0b3 matplotlib pillow
```

```python
from dotenv import load_dotenv
import os
from PIL import Image, ImageDraw
import sys
from matplotlib import pyplot as plt
from azure.core.exceptions import HttpResponseError
import requests
import json
# import namespaces
from azure.ai.vision.imageanalysis import ImageAnalysisClient
from azure.ai.vision.imageanalysis.models import VisualFeatures
from azure.core.credentials import AzureKeyCredential


def main():
    global cv_client

    try:
        # Get Configuration Settings
        load_dotenv()
        ai_endpoint = os.getenv('AI_SERVICE_ENDPOINT')
        ai_key = os.getenv('AI_SERVICE_KEY')

        # Get image
        image_file = 'images/street.jpg'
        if len(sys.argv) > 1:
            image_file = sys.argv[1]

        with open(image_file, "rb") as f:
            image_data = f.read()

        # Authenticate Azure AI Vision client
        cv_client = ImageAnalysisClient(
            endpoint=ai_endpoint,
            credential=AzureKeyCredential(ai_key)
        )


        # Analyze image
        AnalyzeImage(image_file, image_data, cv_client)

        # Background removal
        BackgroundForeground(ai_endpoint, ai_key, image_file)

    except Exception as ex:
        print(ex)


def AnalyzeImage(image_filename, image_data, cv_client):
    print('
Analyzing image...')

    try:
        # Get result with specified features to be retrieved
        result = cv_client.analyze(
            image_data = image_data,
            visual_features = [
                VisualFeatures.CAPTION,
                VisualFeatures.DENSE_CAPTIONS,
                VisualFeatures.TAGS,
                VisualFeatures.OBJECTS,
                VisualFeatures.PEOPLE
            ]
        )

    except HttpResponseError as e:
        print(f"Status code: {e.status_code}")
        print(f"Reason: {e.reason}")
        print(f"Message: {e.error.message}")

    # Display analysis results
    # Get image captions
    if result.caption is not None:
        print("
Caption:")
        print(" Caption: '{}' (confidence: {:.2f}%)".format(result.caption.text, result.caption.confidence * 100))

    # Get image dense captions
    if result.dense_captions is not None:
        print("
Dense Captions:")
        for caption in result.dense_captions.list:
            print(" Caption: '{}' (confidence: {:.2f}%)".format(caption.text, caption.confidence * 100))

    if result.tags is not None:
        print(" Tags:")
        for tag in result.tags.list:
            print("   '{}', Confidence {:.2f}".format(tag.name, tag.confidence * 100.0))

    if result.objects is not None:
        print(" Objects in image:")
        image = Image.open(image_filename)
        fig = plt.figure(figsize = (image.width/100, image.height/ 100))
        plt.axis('off')
        draw = ImageDraw.Draw(image)
        color = 'cyan'
        for detected_object in result.objects.list:
            # Print object name
            print(" {} (confidence: {:.2f}%)".format(detected_object.tags[0].name, detected_object.tags[0].confidence * 100))

            # Draw object bounding box
            r = detected_object.bounding_box
            bounding_box = ((r.x, r.y), (r.x + r.width, r.y + r.height))
            draw.rectangle(bounding_box, outline=color, width=3)
            plt.annotate(detected_object.tags[0].name,(r.x, r.y), backgroundcolor=color)

        # Save annotated image
        plt.imshow(image)
        plt.tight_layout(pad=0)
        outputfile = 'objects.jpg'
        fig.savefig(outputfile)
        print('  Results saved in', outputfile)

    # Get people in the image# Get people in the image
    if result.people is not None:
        print("
People in image:")

        # Prepare image for drawing
        image = Image.open(image_filename)
        fig = plt.figure(figsize=(image.width/100, image.height/100))
        plt.axis('off')
        draw = ImageDraw.Draw(image)
        color = 'cyan'

        for detected_people in result.people.list:
            # Draw object bounding box
            r = detected_people.bounding_box
            bounding_box = ((r.x, r.y), (r.x + r.width, r.y + r.height))
            draw.rectangle(bounding_box, outline=color, width=3)

            # Return the confidence of the person detected
            #print(" {} (confidence: {:.2f}%)".format(detected_people.bounding_box, detected_people.confidence * 100))

        # Save annotated image
        plt.imshow(image)
        plt.tight_layout(pad=0)
        outputfile = 'people.jpg'
        fig.savefig(outputfile)
        print('  Results saved in', outputfile)

def BackgroundForeground(endpoint, key, image_file):
    # Define the API version and mode
    api_version = "2023-02-01-preview"
    mode="backgroundRemoval" # Can be "foregroundMatting" or "backgroundRemoval"

    # Remove the background from the image or generate a foreground matte
    # Remove the background from the image or generate a foreground matte
    print('
Removing background from image...')

    url = "{}computervision/imageanalysis:segment?api-version={}&mode={}".format(endpoint, api_version, mode)

    headers= {
        "Ocp-Apim-Subscription-Key": key,
        "Content-Type": "application/json"
    }

    image_url="https://github.com/MicrosoftLearning/mslearn-ai-vision/blob/main/Labfiles/01-analyze-images/Python/image-analysis/{}?raw=true".format(image_file)

    body = {
        "url": image_url,
    }

    response = requests.post(url, headers=headers, json=body)

    image=response.content
    with open("background.png", "wb") as file:
        file.write(image)
    print('  Results saved in background.png
')



if __name__ == "__main__":
    main()

```

#### image classification:

```powershell
$storageAcct = '<storageAccount>'
(Get-Content training-images/training_labels.json) -replace '<storageAccount>', $storageAcct | Out-File training-images/training_labels.json
```

```json
{
  "images": [
    {
      "id": 1,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164823.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164823.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164823.jpg",
      "date_captured": "2023-12-07T22:52:56.1086527Z"
    },
    {
      "id": 2,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164932.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164932.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164932.jpg",
      "date_captured": "2023-12-07T22:52:56.1086381Z"
    },
    {
      "id": 3,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164957.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164957.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164957.jpg",
      "date_captured": "2023-12-07T22:52:56.1087048Z"
    },
    {
      "id": 4,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165219.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165219.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165219.jpg",
      "date_captured": "2023-12-07T22:52:56.1086937Z"
    },
    {
      "id": 5,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164926jpg.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164926jpg.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164926jpg.jpg",
      "date_captured": "2023-12-07T22:52:56.1086765Z"
    },
    {
      "id": 6,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164925.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164925.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164925.jpg",
      "date_captured": "2023-12-07T22:52:56.1086545Z"
    },
    {
      "id": 7,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164901.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164901.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164901.jpg",
      "date_captured": "2023-12-07T22:52:56.108699Z"
    },
    {
      "id": 8,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165236.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165236.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165236.jpg",
      "date_captured": "2023-12-07T22:52:56.1087188Z"
    },
    {
      "id": 9,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164760.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164760.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164760.jpg",
      "date_captured": "2023-12-07T22:52:56.1087144Z"
    },
    {
      "id": 10,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165108.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165108.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165108.jpg",
      "date_captured": "2023-12-07T22:52:56.1086463Z"
    },
    {
      "id": 11,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165220.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165220.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165220.jpg",
      "date_captured": "2023-12-07T22:52:56.1087029Z"
    },
    {
      "id": 12,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165232.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165232.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165232.jpg",
      "date_captured": "2023-12-07T22:52:56.1086564Z"
    },
    {
      "id": 13,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164918.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164918.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164918.jpg",
      "date_captured": "2023-12-07T22:52:56.1086359Z"
    },
    {
      "id": 14,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165152.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165152.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165152.jpg",
      "date_captured": "2023-12-07T22:52:56.1086681Z"
    },
    {
      "id": 15,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165157.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165157.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165157.jpg",
      "date_captured": "2023-12-07T22:52:56.1087245Z"
    },
    {
      "id": 16,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164830.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164830.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164830.jpg",
      "date_captured": "2023-12-07T22:52:56.108666Z"
    },
    {
      "id": 17,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165112.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165112.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165112.jpg",
      "date_captured": "2023-12-07T22:52:56.1086872Z"
    },
    {
      "id": 18,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165021.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165021.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165021.jpg",
      "date_captured": "2023-12-07T22:52:56.1086916Z"
    },
    {
      "id": 19,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165001.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165001.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165001.jpg",
      "date_captured": "2023-12-07T22:52:56.1086442Z"
    },
    {
      "id": 20,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164952.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164952.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164952.jpg",
      "date_captured": "2023-12-07T22:52:56.1086703Z"
    },
    {
      "id": 21,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165026.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165026.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165026.jpg",
      "date_captured": "2023-12-07T22:52:56.1086635Z"
    },
    {
      "id": 22,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164947.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164947.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164947.jpg",
      "date_captured": "2023-12-07T22:52:56.1086895Z"
    },
    {
      "id": 23,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165002.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165002.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165002.jpg",
      "date_captured": "2023-12-07T22:52:56.1086831Z"
    },
    {
      "id": 24,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164958.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164958.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164958.jpg",
      "date_captured": "2023-12-07T22:52:56.1086614Z"
    },
    {
      "id": 25,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164936.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164936.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164936.jpg",
      "date_captured": "2023-12-07T22:52:56.1086341Z"
    },
    {
      "id": 26,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165115.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165115.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165115.jpg",
      "date_captured": "2023-12-07T22:52:56.108701Z"
    },
    {
      "id": 27,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164811.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164811.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164811.jpg",
      "date_captured": "2023-12-07T22:52:56.1087166Z"
    },
    {
      "id": 28,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165027.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165027.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165027.jpg",
      "date_captured": "2023-12-07T22:52:56.1086507Z"
    },
    {
      "id": 29,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164804.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164804.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164804.jpg",
      "date_captured": "2023-12-07T22:52:56.1086321Z"
    },
    {
      "id": 30,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165047.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165047.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165047.jpg",
      "date_captured": "2023-12-07T22:52:56.1086851Z"
    },
    {
      "id": 31,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165016.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165016.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165016.jpg",
      "date_captured": "2023-12-07T22:52:56.1087066Z"
    },
    {
      "id": 32,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164759.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164759.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164759.jpg",
      "date_captured": "2023-12-07T22:52:56.1087109Z"
    },
    {
      "id": 33,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164819.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164819.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164819.jpg",
      "date_captured": "2023-12-07T22:52:56.1087209Z"
    },
    {
      "id": 34,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165126.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165126.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165126.jpg",
      "date_captured": "2023-12-07T22:52:56.1086263Z"
    },
    {
      "id": 35,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165234.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165234.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165234.jpg",
      "date_captured": "2023-12-07T22:52:56.1086404Z"
    },
    {
      "id": 36,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164919.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164919.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164919.jpg",
      "date_captured": "2023-12-07T22:52:56.1086486Z"
    },
    {
      "id": 37,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165147.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165147.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165147.jpg",
      "date_captured": "2023-12-07T22:52:56.1086969Z"
    },
    {
      "id": 38,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165202.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165202.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165202.jpg",
      "date_captured": "2023-12-07T22:52:56.1086813Z"
    },
    {
      "id": 39,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165223.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165223.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165223.jpg",
      "date_captured": "2023-12-07T22:52:56.1087088Z"
    },
    {
      "id": 40,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165046.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165046.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165046.jpg",
      "date_captured": "2023-12-07T22:52:56.1086724Z"
    },
    {
      "id": 41,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165020.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165020.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165020.jpg",
      "date_captured": "2023-12-07T22:52:56.1086794Z"
    },
    {
      "id": 42,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165033.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165033.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165033.jpg",
      "date_captured": "2023-12-07T22:52:56.1086746Z"
    },
    {
      "id": 43,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165132.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165132.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165132.jpg",
      "date_captured": "2023-12-07T22:52:56.1087227Z"
    },
    {
      "id": 44,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165008.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165008.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165008.jpg",
      "date_captured": "2023-12-07T22:52:56.1086582Z"
    },
    {
      "id": 45,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164851.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164851.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164851.jpg",
      "date_captured": "2023-12-07T22:52:56.1086301Z"
    }
  ],
  "annotations": [
    {
      "id": 1,
      "category_id": 1,
      "image_id": 1,
      "area": 0.0
    },
    {
      "id": 2,
      "category_id": 1,
      "image_id": 2,
      "area": 0.0
    },
    {
      "id": 3,
      "category_id": 3,
      "image_id": 3,
      "area": 0.0
    },
    {
      "id": 4,
      "category_id": 2,
      "image_id": 4,
      "area": 0.0
    },
    {
      "id": 5,
      "category_id": 1,
      "image_id": 5,
      "area": 0.0
    },
    {
      "id": 6,
      "category_id": 1,
      "image_id": 6,
      "area": 0.0
    },
    {
      "id": 7,
      "category_id": 1,
      "image_id": 7,
      "area": 0.0
    },
    {
      "id": 8,
      "category_id": 2,
      "image_id": 8,
      "area": 0.0
    },
    {
      "id": 9,
      "category_id": 1,
      "image_id": 9,
      "area": 0.0
    },
    {
      "id": 10,
      "category_id": 2,
      "image_id": 10,
      "area": 0.0
    },
    {
      "id": 11,
      "category_id": 2,
      "image_id": 11,
      "area": 0.0
    },
    {
      "id": 12,
      "category_id": 2,
      "image_id": 12,
      "area": 0.0
    },
    {
      "id": 13,
      "category_id": 1,
      "image_id": 13,
      "area": 0.0
    },
    {
      "id": 14,
      "category_id": 2,
      "image_id": 14,
      "area": 0.0
    },
    {
      "id": 15,
      "category_id": 2,
      "image_id": 15,
      "area": 0.0
    },
    {
      "id": 16,
      "category_id": 1,
      "image_id": 16,
      "area": 0.0
    },
    {
      "id": 17,
      "category_id": 2,
      "image_id": 17,
      "area": 0.0
    },
    {
      "id": 18,
      "category_id": 3,
      "image_id": 18,
      "area": 0.0
    },
    {
      "id": 19,
      "category_id": 3,
      "image_id": 19,
      "area": 0.0
    },
    {
      "id": 20,
      "category_id": 3,
      "image_id": 20,
      "area": 0.0
    },
    {
      "id": 21,
      "category_id": 3,
      "image_id": 21,
      "area": 0.0
    },
    {
      "id": 22,
      "category_id": 3,
      "image_id": 22,
      "area": 0.0
    },
    {
      "id": 23,
      "category_id": 3,
      "image_id": 23,
      "area": 0.0
    },
    {
      "id": 24,
      "category_id": 3,
      "image_id": 24,
      "area": 0.0
    },
    {
      "id": 25,
      "category_id": 1,
      "image_id": 25,
      "area": 0.0
    },
    {
      "id": 26,
      "category_id": 2,
      "image_id": 26,
      "area": 0.0
    },
    {
      "id": 27,
      "category_id": 1,
      "image_id": 27,
      "area": 0.0
    },
    {
      "id": 28,
      "category_id": 3,
      "image_id": 28,
      "area": 0.0
    },
    {
      "id": 29,
      "category_id": 1,
      "image_id": 29,
      "area": 0.0
    },
    {
      "id": 30,
      "category_id": 3,
      "image_id": 30,
      "area": 0.0
    },
    {
      "id": 31,
      "category_id": 3,
      "image_id": 31,
      "area": 0.0
    },
    {
      "id": 32,
      "category_id": 1,
      "image_id": 32,
      "area": 0.0
    },
    {
      "id": 33,
      "category_id": 1,
      "image_id": 33,
      "area": 0.0
    },
    {
      "id": 34,
      "category_id": 2,
      "image_id": 34,
      "area": 0.0
    },
    {
      "id": 35,
      "category_id": 2,
      "image_id": 35,
      "area": 0.0
    },
    {
      "id": 36,
      "category_id": 1,
      "image_id": 36,
      "area": 0.0
    },
    {
      "id": 37,
      "category_id": 2,
      "image_id": 37,
      "area": 0.0
    },
    {
      "id": 38,
      "category_id": 2,
      "image_id": 38,
      "area": 0.0
    },
    {
      "id": 39,
      "category_id": 2,
      "image_id": 39,
      "area": 0.0
    },
    {
      "id": 40,
      "category_id": 3,
      "image_id": 40,
      "area": 0.0
    },
    {
      "id": 41,
      "category_id": 3,
      "image_id": 41,
      "area": 0.0
    },
    {
      "id": 42,
      "category_id": 3,
      "image_id": 42,
      "area": 0.0
    },
    {
      "id": 43,
      "category_id": 2,
      "image_id": 43,
      "area": 0.0
    },
    {
      "id": 44,
      "category_id": 3,
      "image_id": 44,
      "area": 0.0
    },
    {
      "id": 45,
      "category_id": 1,
      "image_id": 45,
      "area": 0.0
    }
  ],
  "categories": [
    {
      "id": 1,
      "name": "apple"
    },
    {
      "id": 2,
      "name": "orange"
    },
    {
      "id": 3,
      "name": "banana"
    }
  ]
}
```

#### object detection:

##### test-detector:

```python
from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient
from msrest.authentication import ApiKeyCredentials
from matplotlib import pyplot as plt
from PIL import Image, ImageDraw, ImageFont
import numpy as np
import os

def main():
    from dotenv import load_dotenv

    try:
        # Get Configuration Settings
        load_dotenv()
        prediction_endpoint = os.getenv('PredictionEndpoint')
        prediction_key = os.getenv('PredictionKey')
        project_id = os.getenv('ProjectID')
        model_name = os.getenv('ModelName')

        # Authenticate a client for the training API
        credentials = ApiKeyCredentials(in_headers={"Prediction-key": prediction_key})
        prediction_client = CustomVisionPredictionClient(endpoint=prediction_endpoint, credentials=credentials)

        # Load image and get height, width and channels
        image_file = 'produce.jpg'
        print('Detecting objects in', image_file)
        image = Image.open(image_file)
        h, w, ch = np.array(image).shape

        # Detect objects in the test image
        with open(image_file, mode="rb") as image_data:
            results = prediction_client.detect_image(project_id, model_name, image_data)

        # Create a figure for the results
        fig = plt.figure(figsize=(8, 8))
        plt.axis('off')

        # Display the image with boxes around each detected object
        draw = ImageDraw.Draw(image)
        lineWidth = int(w/100)
        color = 'magenta'
        for prediction in results.predictions:
            # Only show objects with a > 50% probability
            if (prediction.probability*100) > 50:
                # Box coordinates and dimensions are proportional - convert to absolutes
                left = prediction.bounding_box.left * w
                top = prediction.bounding_box.top * h
                height = prediction.bounding_box.height * h
                width =  prediction.bounding_box.width * w
                # Draw the box
                points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))
                draw.line(points, fill=color, width=lineWidth)
                # Add the tag name and probability
                plt.annotate(prediction.tag_name + ": {0:.2f}%".format(prediction.probability * 100),(left,top), backgroundcolor=color)
        plt.imshow(image)
        outputfile = 'output.jpg'
        fig.savefig(outputfile)
        print('Results saved in ', outputfile)
    except Exception as ex:
        print(ex)

if __name__ == "__main__":
    main()

```

```dotenv
PredictionEndpoint=YOUR_PREDICTION_ENDPOINT
PredictionKey=YOUR_PREDICTION_KEY
ProjectID=YOUR_PROJECT_ID
ModelName=fruit-detector
```

##### Train detector:

```dotenv
TrainingEndpoint=YOUR_TRAINING_ENDPOINT
TrainingKey=YOUR_TRAINING_KEY
ProjectID=YOUR_PROJECT_ID
```

```python
from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient
from azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region
from msrest.authentication import ApiKeyCredentials
import time
import json
import os

def main():
    from dotenv import load_dotenv
    global training_client
    global custom_vision_project

    try:
        # Get Configuration Settings
        load_dotenv()
        training_endpoint = os.getenv('TrainingEndpoint')
        training_key = os.getenv('TrainingKey')
        project_id = os.getenv('ProjectID')

        # Authenticate a client for the training API
        credentials = ApiKeyCredentials(in_headers={"Training-key": training_key})
        training_client = CustomVisionTrainingClient(training_endpoint, credentials)

        # Get the Custom Vision project
        custom_vision_project = training_client.get_project(project_id)

        # Upload and tag images
        Upload_Images('images')
    except Exception as ex:
        print(ex)



def Upload_Images(folder):
    print("Uploading images...")

    # Get the tags defined in the project
    tags = training_client.get_tags(custom_vision_project.id)

    # Create a list of images with tagged regions
    tagged_images_with_regions = []

    # Get the images and tagged regions from the JSON file
    with open('tagged-images.json', 'r') as json_file:
        tagged_images = json.load(json_file)
        for image in tagged_images['files']:
            # Get the filename
            file = image['filename']
            # Get the tagged regions
            regions = []
            for tag in image['tags']:
                tag_name = tag['tag']
                # Look up the tag ID for this tag name
                tag_id = next(t for t in tags if t.name == tag_name).id
                # Add a region for this tag using the coordinates and dimensions in the JSON
                regions.append(Region(tag_id=tag_id, left=tag['left'],top=tag['top'],width=tag['width'],height=tag['height']))
            # Add the image and its regions to the list
            with open(os.path.join(folder,file), mode="rb") as image_data:
                tagged_images_with_regions.append(ImageFileCreateEntry(name=file, contents=image_data.read(), regions=regions))

    # Upload the list of images as a batch
    upload_result = training_client.create_images_from_files(custom_vision_project.id, ImageFileCreateBatch(images=tagged_images_with_regions))
    # Check for failure
    if not upload_result.is_batch_successful:
        print("Image batch upload failed.")
        for image in upload_result.images:
            print("Image status: ", image.status)
    else:
        print("Images uploaded.")

if __name__ == "__main__":
    main()
```

train image json

```json
{
  "files": [
    {
      "filename": "image11.jpg",
      "tags": [
        {
          "tag": "orange",
          "left": 0.501782656,
          "top": 0.141307935,
          "width": 0.30014348,
          "height": 0.421263933
        },
        {
          "tag": "banana",
          "left": 0.117563143,
          "top": 0.269699484,
          "width": 0.4181828,
          "height": 0.629577756
        }
      ]
    },
    {
      "filename": "image12.jpg",
      "tags": [
        {
          "tag": "banana",
          "left": 0.0266054422,
          "top": 0.429971635,
          "width": 0.9334502,
          "height": 0.5663317
        },
        {
          "tag": "apple",
          "left": 0.594586432,
          "top": 0.139771029,
          "width": 0.2337932,
          "height": 0.313067973
        },
        {
          "tag": "orange",
          "left": 0.24538058,
          "top": 0.06600542,
          "width": 0.230479121,
          "height": 0.3283311
        }
      ]
    },
    {
      "filename": "image13.jpg",
      "tags": [
        {
          "tag": "orange",
          "left": 0.508894742,
          "top": 0.155026585,
          "width": 0.370887518,
          "height": 0.512516
        },
        {
          "tag": "banana",
          "left": 0.08663659,
          "top": 0.253976226,
          "width": 0.382285058,
          "height": 0.584964633
        }
      ]
    },
    {
      "filename": "image14.jpg",
      "tags": [
        {
          "tag": "apple",
          "left": 0.6080398,
          "top": 0.222880378,
          "width": 0.3454436,
          "height": 0.498601437
        },
        {
          "tag": "banana",
          "left": 0.08858037,
          "top": 0.2583913,
          "width": 0.394662261,
          "height": 0.6532483
        }
      ]
    },
    {
      "filename": "image15.jpg",
      "tags": [
        {
          "tag": "orange",
          "left": 0.124889567,
          "top": 0.160744965,
          "width": 0.365462124,
          "height": 0.503556669
        },
        {
          "tag": "banana",
          "left": 0.530725956,
          "top": 0.261967421,
          "width": 0.380674481,
          "height": 0.5697762
        }
      ]
    },
    {
      "filename": "image16.jpg",
      "tags": [
        {
          "tag": "banana",
          "left": 0.24473004,
          "top": 0.316017568,
          "width": 0.621397436,
          "height": 0.62547636
        },
        {
          "tag": "apple",
          "left": 0.2669289,
          "top": 0.1570937,
          "width": 0.2849568,
          "height": 0.373458445
        }
      ]
    },
    {
      "filename": "image17.jpg",
      "tags": [
        {
          "tag": "banana",
          "left": 0.137872428,
          "top": 0.3037218,
          "width": 0.6171047,
          "height": 0.6428385
        },
        {
          "tag": "apple",
          "left": 0.446089834,
          "top": 0.158026949,
          "width": 0.2905319,
          "height": 0.368484139
        }
      ]
    },
    {
      "filename": "image18.jpg",
      "tags": [
        {
          "tag": "banana",
          "left": 0.345145166,
          "top": 0.315790027,
          "width": 0.389697134,
          "height": 0.5641991
        },
        {
          "tag": "orange",
          "left": 0.6284472,
          "top": 0.160571277,
          "width": 0.25890553,
          "height": 0.321568727
        },
        {
          "tag": "orange",
          "left": 0.0472412556,
          "top": 0.317386866,
          "width": 0.344220281,
          "height": 0.451182485
        }
      ]
    },
    {
      "filename": "image19.jpg",
      "tags": [
        {
          "tag": "orange",
          "left": 0.190300852,
          "top": 0.04908291,
          "width": 0.6819816,
          "height": 0.89233005
        }
      ]
    },
    {
      "filename": "image20.jpg",
      "tags": [
        {
          "tag": "orange",
          "left": 0.296974,
          "top": 0.11914885,
          "width": 0.467207551,
          "height": 0.6827575
        }
      ]
    },
    {
      "filename": "image21.jpg",
      "tags": [
        {
          "tag": "orange",
          "left": 0.235896885,
          "top": 0.144391567,
          "width": 0.5458362,
          "height": 0.739897847
        }
      ]
    },
    {
      "filename": "image22.jpg",
      "tags": [
        {
          "tag": "orange",
          "left": 0.242359161,
          "top": 0.039894,
          "width": 0.632171631,
          "height": 0.903954148
        }
      ]
    },
    {
      "filename": "image23.jpg",
      "tags": [
        {
          "tag": "orange",
          "left": 0.242662221,
          "top": 0.06341483,
          "width": 0.670523345,
          "height": 0.7936373
        }
      ]
    },
    {
      "filename": "image24.jpg",
      "tags": [
        {
          "tag": "banana",
          "left": 0.06202866,
          "top": 0.319153845,
          "width": 0.9132734,
          "height": 0.640570164
        }
      ]
    },
    {
      "filename": "image25.jpg",
      "tags": [
        {
          "tag": "banana",
          "left": 0.234958112,
          "top": 0.165271968,
          "width": 0.51941216,
          "height": 0.77990067
        }
      ]
    },
    {
      "filename": "image26.jpg",
      "tags": [
        {
          "tag": "banana",
          "left": 0.123737864,
          "top": 0.311502129,
          "width": 0.6994619,
          "height": 0.6375427
        }
      ]
    },
    {
      "filename": "image27.jpg",
      "tags": [
        {
          "tag": "banana",
          "left": 0.147404462,
          "top": 0.332391232,
          "width": 0.708104849,
          "height": 0.41904977
        }
      ]
    },
    {
      "filename": "image28.jpg",
      "tags": [
        {
          "tag": "banana",
          "left": 0.294261217,
          "top": 0.262935251,
          "width": 0.476716518,
          "height": 0.7041446
        }
      ]
    },
    {
      "filename": "image29.jpg",
      "tags": [
        {
          "tag": "apple",
          "left": 0.152405351,
          "top": 0.105940014,
          "width": 0.598258257,
          "height": 0.8126528
        }
      ]
    },
    {
      "filename": "image30.jpg",
      "tags": [
        {
          "tag": "apple",
          "left": 0.255153179,
          "top": 0.1485295,
          "width": 0.528649,
          "height": 0.7251674
        }
      ]
    },
    {
      "filename": "image31.jpg",
      "tags": [
        {
          "tag": "apple",
          "left": 0.227940559,
          "top": 0.146328539,
          "width": 0.561518431,
          "height": 0.767682433
        }
      ]
    },
    {
      "filename": "image32.jpg",
      "tags": [
        {
          "tag": "apple",
          "left": 0.18220368,
          "top": 0.0870502,
          "width": 0.602320552,
          "height": 0.7840438
        }
      ]
    },
    {
      "filename": "image33.jpg",
      "tags": [
        {
          "tag": "apple",
          "left": 0.2639115,
          "top": 0.153765231,
          "width": 0.55621016,
          "height": 0.7570554
        }
      ]
    }
  ]
}
```

#### face:

##### computer vision:

```dotenv
AI_SERVICE_ENDPOINT=your_ai_services_endpoint
AI_SERVICE_KEY=your_ai_services_key
```

```python
from dotenv import load_dotenv
import os
from PIL import Image, ImageDraw
import sys
from matplotlib import pyplot as plt
import numpy as np

# import namespaces



def main():
    global cv_client

    try:
        # Get Configuration Settings
        load_dotenv()
        ai_endpoint = os.getenv('AI_SERVICE_ENDPOINT')
        ai_key = os.getenv('AI_SERVICE_KEY')

        # Get image
        image_file = 'images/people.jpg'
        if len(sys.argv) > 1:
            image_file = sys.argv[1]

        with open(image_file, "rb") as f:
            image_data = f.read()

        # Authenticate Azure AI Vision client


        # Analyze image
        AnalyzeImage(image_file, image_data, cv_client)

    except Exception as ex:
        print(ex)


def AnalyzeImage(filename, image_data, cv_client):
    print('
Analyzing ', filename)

    # Get result with specified features to be retrieved (PEOPLE)


    # Identify people in the image
    if result.people is not None:
        print("
People in image:")

        # Prepare image for drawing
        image = Image.open(filename)
        fig = plt.figure(figsize=(image.width/100, image.height/100))
        plt.axis('off')
        draw = ImageDraw.Draw(image)
        color = 'cyan'

        # Draw bounding box around detected people


        # Save annotated image
        plt.imshow(image)
        plt.tight_layout(pad=0)
        outputfile = 'people.jpg'
        fig.savefig(outputfile)
        print('  Results saved in', outputfile)

if __name__ == "__main__":
    main()
```

##### face api:

```python
from dotenv import load_dotenv
import os
from PIL import Image, ImageDraw
from matplotlib import pyplot as plt

# Import namespaces


def main():

    global face_client

    try:
        # Get Configuration Settings
        load_dotenv()
        cog_endpoint = os.getenv('AI_SERVICE_ENDPOINT')
        cog_key = os.getenv('AI_SERVICE_KEY')

        # Authenticate Face client


        # Menu for face functions
        print('1: Detect faces
Any other key to quit')
        command = input('Enter a number:')
        if command == '1':
            DetectFaces(os.path.join('images','people.jpg'))

    except Exception as ex:
        print(ex)

def DetectFaces(image_file):
    print('Detecting faces in', image_file)

    # Specify facial features to be retrieved


    # Get faces


if __name__ == "__main__":
    main()
```

```dotenv
AI_SERVICE_ENDPOINT=your_ai_services_endpoint
AI_SERVICE_KEY=your_ai_services_key
```

#### ocr:

```python
from dotenv import load_dotenv
import os
import time
from PIL import Image, ImageDraw
from matplotlib import pyplot as plt

# Import namespaces


def main():

    global cv_client

    try:
        # Get Configuration Settings
        load_dotenv()
        ai_endpoint = os.getenv('AI_SERVICE_ENDPOINT')
        ai_key = os.getenv('AI_SERVICE_KEY')

        # Authenticate Azure AI Vision client


        # Menu for text reading functions
        print('
1: Use Read API for image (Lincoln.jpg)
2: Read handwriting (Note.jpg)
Any other key to quit
')
        command = input('Enter a number:')
        if command == '1':
            image_file = os.path.join('images','Lincoln.jpg')
            GetTextRead(image_file)
        elif command =='2':
            image_file = os.path.join('images','Note.jpg')
            GetTextRead(image_file)


    except Exception as ex:
        print(ex)

def GetTextRead(image_file):
    print('
')

    # Open image file
    with open(image_file, "rb") as f:
            image_data = f.read()

    # Use Analyze image function to read text in image





if __name__ == "__main__":
    main()

```

```dotenv
AI_SERVICE_ENDPOINT=your_ai_services_endpoint
AI_SERVICE_KEY=your_ai_services_key
```

#### video indexer:

```powershell
$account_id="YOUR_ACCOUNT_ID"
$api_key="YOUR_API_KEY"
$location="trial"

# Call the AccessToken method with the API key in the header to get an access token
$token = Invoke-RestMethod -Method "Get" -Uri "https://api.videoindexer.ai/auth/$location/Accounts/$account_id/AccessToken" -Headers @{'Ocp-Apim-Subscription-Key' = $api_key}

# Use the access token to make an authenticated call to the Videos method to get a list of videos in the account
Invoke-RestMethod -Method "Get" -Uri "https://api.videoindexer.ai/$location/Accounts/$account_id/Videos?accessToken=$token" | ConvertTo-Json -Depth 6

```

```html
<html>
  <header>
    <title>Analyze Video</title>
    <script src="https://breakdown.blob.core.windows.net/public/vb.widgets.mediator.js"></script>
  </header>
  <body>
    <h1>Video Analysis</h1>
    <table>
      <tr>
        <td style="vertical-align:top;">
          <!--Player widget goes here -->
        </td>
        <td style="vertical-align:top;">
          <!-- Insights widget goes here -->
        </td>
      </tr>
    </table>
  </body>
</html>
```

#### custom vision image classification:

##### test-images:

```python
from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient
from msrest.authentication import ApiKeyCredentials
import os

def main():
    from dotenv import load_dotenv

    try:
        # Get Configuration Settings
        load_dotenv()
        prediction_endpoint = os.getenv('PredictionEndpoint')
        prediction_key = os.getenv('PredictionKey')
        project_id = os.getenv('ProjectID')
        model_name = os.getenv('ModelName')

        # Authenticate a client for the training API
        credentials = ApiKeyCredentials(in_headers={"Prediction-key": prediction_key})
        prediction_client = CustomVisionPredictionClient(endpoint=prediction_endpoint, credentials=credentials)

        # Classify test images
        for image in os.listdir('test-images'):
            image_data = open(os.path.join('test-images',image), "rb").read()
            results = prediction_client.classify_image(project_id, model_name, image_data)

            # Loop over each label prediction and print any with probability > 50%
            for prediction in results.predictions:
                if prediction.probability > 0.5:
                    print(image, ': {} ({:.0%})'.format(prediction.tag_name, prediction.probability))
    except Exception as ex:
        print(ex)

if __name__ == "__main__":
    main()


```

```dotenv
PredictionEndpoint=YOUR_PREDICTION_ENDPOINT
PredictionKey=YOUR_PREDICTION_KEY
ProjectID=YOUR_PROJECT_ID
ModelName=fruit-classifier
```

##### train-classifier:

```dotenv
TrainingEndpoint=YOUR_TRAINING_ENDPOINT
TrainingKey=YOUR_TRAINING_KEY
ProjectID=YOUR_PROJECT_ID
```

```python
from azure.cognitiveservices.vision.customvision.training import CustomVisionTrainingClient
from azure.cognitiveservices.vision.customvision.training.models import ImageFileCreateBatch, ImageFileCreateEntry, Region
from msrest.authentication import ApiKeyCredentials
import time
import os

def main():
    from dotenv import load_dotenv
    global training_client
    global custom_vision_project

    try:
        # Get Configuration Settings
        load_dotenv()
        training_endpoint = os.getenv('TrainingEndpoint')
        training_key = os.getenv('TrainingKey')
        project_id = os.getenv('ProjectID')

        # Authenticate a client for the training API
        credentials = ApiKeyCredentials(in_headers={"Training-key": training_key})
        training_client = CustomVisionTrainingClient(training_endpoint, credentials)

        # Get the Custom Vision project
        custom_vision_project = training_client.get_project(project_id)

        # Upload and tag images
        Upload_Images('more-training-images')

        # Train the model
        Train_Model()

    except Exception as ex:
        print(ex)

def Upload_Images(folder):
    print("Uploading images...")
    tags = training_client.get_tags(custom_vision_project.id)
    for tag in tags:
        print(tag.name)
        for image in os.listdir(os.path.join(folder,tag.name)):
            image_data = open(os.path.join(folder,tag.name,image), "rb").read()
            training_client.create_images_from_data(custom_vision_project.id, image_data, [tag.id])

def Train_Model():
    print("Training ...")
    iteration = training_client.train_project(custom_vision_project.id)
    while (iteration.status != "Completed"):
        iteration = training_client.get_iteration(custom_vision_project.id, iteration.id)
        print (iteration.status, '...')
        time.sleep(5)
    print ("Model trained!")


if __name__ == "__main__":
    main()

```

### Lab 3(Read Text and Images):

OCR:

```bash
pip install azure-ai-vision-imageanalysis==1.0.0b3
```

```python
from dotenv import load_dotenv
import os
import time
from PIL import Image, ImageDraw
from matplotlib import pyplot as plt

# Import namespaces
from azure.ai.vision.imageanalysis import ImageAnalysisClient
from azure.ai.vision.imageanalysis.models import VisualFeatures
from azure.core.credentials import AzureKeyCredential


def main():

    global cv_client

    try:
        # Get Configuration Settings
        load_dotenv()
        ai_endpoint = os.getenv('AI_SERVICE_ENDPOINT')
        ai_key = os.getenv('AI_SERVICE_KEY')

        # Authenticate Azure AI Vision client
        # Authenticate Azure AI Vision client
        cv_client = ImageAnalysisClient(endpoint=ai_endpoint,
            credential=AzureKeyCredential(ai_key))
        # Menu for text reading functions
        print('
1: Use Read API for image (Lincoln.jpg)
2: Read handwriting (Note.jpg)
Any other key to quit
')
        command = input('Enter a number:')
        if command == '1':
            image_file = os.path.join('images','Lincoln.jpg')
            GetTextRead(image_file)
        elif command =='2':
            image_file = os.path.join('images','Note.jpg')
            GetTextRead(image_file)


    except Exception as ex:
        print(ex)

def GetTextRead(image_file):
    print('
')

    # Open image file
    with open(image_file, "rb") as f:
        image_data = f.read()

    # Use Analyze image function to read text in image
    result = cv_client.analyze(
        image_data=image_data,
        visual_features=[VisualFeatures.READ]
    )

    # Display the image and overlay it with the extracted text
    if result.read is not None:
        print("
Text:")# Prepare image for drawing
    image = Image.open(image_file)
    fig = plt.figure(figsize=(image.width/100, image.height/100))
    plt.axis('off')
    draw = ImageDraw.Draw(image)
    color = 'cyan'

    for line in result.read.blocks[0].lines:
        # Return the text detected in the image
        print(f"  {line.text}")

        drawLinePolygon = True

        r = line.bounding_polygon
        bounding_polygon = ((r[0].x, r[0].y),(r[1].x, r[1].y),(r[2].x, r[2].y),(r[3].x, r[3].y))

        # Return the position bounding box around each line
        print("   Bounding Polygon: {}".format(bounding_polygon))

        # Return each word detected in the image and the position bounding box around each word with the confidence level of each word


        # Draw line bounding polygon
        if drawLinePolygon:
            draw.polygon(bounding_polygon, outline=color, width=3)

    # Save image
    plt.imshow(image)
    plt.tight_layout(pad=0)
    outputfile = 'text.jpg'
    fig.savefig(outputfile)
    print('
  Results saved in', outputfile)


if __name__ == "__main__":
    main()

```

```dotenv
AI_SERVICE_ENDPOINT="https://azure-ai-demo-vision-image-mm.cognitiveservices.azure.com/"
AI_SERVICE_KEY="FQAdhXyGegOk2trbMmXkITsJc5M5VHH6DCKevPl7MuAIKr3QrifyJQQJ99AKACYeBjFXJ3w3AAAAACOGY6JX"
```

### Lab 4 (Classify images with a Azure AI Vision custom model):

#### image classification:

We also need a storage account to store the training images.

- In Azure portal, search for and select Storage accounts, and create a new storage account with the following settings:

  - Subscription: Your Azure subscription
  - Resource Group: Choose the same resource group you created your Azure AI Service resource in
  - Storage Account Name: customclassifySUFFIX
  - note: replace the SUFFIX token with your initials or another value to ensure the resource name is globally unique.
  - Region: Choose the same region you used for your Azure AI Service resource
  - Primary service: Azure Blob Storage or Azure Data Lake Storage Gen 2
  - Primary workload: Other
  - Performance: Standard
  - Redundancy: Locally-redundant storage (LRS)

- replace storage_account account in json file using this script

```powershell
$storageAcct = '<storageAccount>'
(Get-Content training-images/training_labels.json) -replace '<storageAccount>', $storageAcct | Out-File training-images/training_labels.json
```

- training_lables.json

```json
{
  "images": [
    {
      "id": 1,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164823.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164823.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164823.jpg",
      "date_captured": "2023-12-07T22:52:56.1086527Z"
    },
    {
      "id": 2,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164932.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164932.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164932.jpg",
      "date_captured": "2023-12-07T22:52:56.1086381Z"
    },
    {
      "id": 3,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164957.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164957.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164957.jpg",
      "date_captured": "2023-12-07T22:52:56.1087048Z"
    },
    {
      "id": 4,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165219.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165219.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165219.jpg",
      "date_captured": "2023-12-07T22:52:56.1086937Z"
    },
    {
      "id": 5,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164926jpg.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164926jpg.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164926jpg.jpg",
      "date_captured": "2023-12-07T22:52:56.1086765Z"
    },
    {
      "id": 6,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164925.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164925.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164925.jpg",
      "date_captured": "2023-12-07T22:52:56.1086545Z"
    },
    {
      "id": 7,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164901.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164901.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164901.jpg",
      "date_captured": "2023-12-07T22:52:56.108699Z"
    },
    {
      "id": 8,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165236.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165236.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165236.jpg",
      "date_captured": "2023-12-07T22:52:56.1087188Z"
    },
    {
      "id": 9,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164760.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164760.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164760.jpg",
      "date_captured": "2023-12-07T22:52:56.1087144Z"
    },
    {
      "id": 10,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165108.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165108.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165108.jpg",
      "date_captured": "2023-12-07T22:52:56.1086463Z"
    },
    {
      "id": 11,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165220.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165220.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165220.jpg",
      "date_captured": "2023-12-07T22:52:56.1087029Z"
    },
    {
      "id": 12,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165232.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165232.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165232.jpg",
      "date_captured": "2023-12-07T22:52:56.1086564Z"
    },
    {
      "id": 13,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164918.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164918.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164918.jpg",
      "date_captured": "2023-12-07T22:52:56.1086359Z"
    },
    {
      "id": 14,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165152.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165152.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165152.jpg",
      "date_captured": "2023-12-07T22:52:56.1086681Z"
    },
    {
      "id": 15,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165157.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165157.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165157.jpg",
      "date_captured": "2023-12-07T22:52:56.1087245Z"
    },
    {
      "id": 16,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164830.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164830.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164830.jpg",
      "date_captured": "2023-12-07T22:52:56.108666Z"
    },
    {
      "id": 17,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165112.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165112.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165112.jpg",
      "date_captured": "2023-12-07T22:52:56.1086872Z"
    },
    {
      "id": 18,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165021.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165021.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165021.jpg",
      "date_captured": "2023-12-07T22:52:56.1086916Z"
    },
    {
      "id": 19,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165001.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165001.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165001.jpg",
      "date_captured": "2023-12-07T22:52:56.1086442Z"
    },
    {
      "id": 20,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164952.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164952.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164952.jpg",
      "date_captured": "2023-12-07T22:52:56.1086703Z"
    },
    {
      "id": 21,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165026.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165026.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165026.jpg",
      "date_captured": "2023-12-07T22:52:56.1086635Z"
    },
    {
      "id": 22,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164947.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164947.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164947.jpg",
      "date_captured": "2023-12-07T22:52:56.1086895Z"
    },
    {
      "id": 23,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165002.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165002.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165002.jpg",
      "date_captured": "2023-12-07T22:52:56.1086831Z"
    },
    {
      "id": 24,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164958.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164958.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164958.jpg",
      "date_captured": "2023-12-07T22:52:56.1086614Z"
    },
    {
      "id": 25,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164936.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164936.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164936.jpg",
      "date_captured": "2023-12-07T22:52:56.1086341Z"
    },
    {
      "id": 26,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165115.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165115.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165115.jpg",
      "date_captured": "2023-12-07T22:52:56.108701Z"
    },
    {
      "id": 27,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164811.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164811.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164811.jpg",
      "date_captured": "2023-12-07T22:52:56.1087166Z"
    },
    {
      "id": 28,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165027.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165027.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165027.jpg",
      "date_captured": "2023-12-07T22:52:56.1086507Z"
    },
    {
      "id": 29,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164804.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164804.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164804.jpg",
      "date_captured": "2023-12-07T22:52:56.1086321Z"
    },
    {
      "id": 30,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165047.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165047.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165047.jpg",
      "date_captured": "2023-12-07T22:52:56.1086851Z"
    },
    {
      "id": 31,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165016.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165016.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165016.jpg",
      "date_captured": "2023-12-07T22:52:56.1087066Z"
    },
    {
      "id": 32,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164759.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164759.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164759.jpg",
      "date_captured": "2023-12-07T22:52:56.1087109Z"
    },
    {
      "id": 33,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164819.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164819.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164819.jpg",
      "date_captured": "2023-12-07T22:52:56.1087209Z"
    },
    {
      "id": 34,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165126.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165126.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165126.jpg",
      "date_captured": "2023-12-07T22:52:56.1086263Z"
    },
    {
      "id": 35,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165234.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165234.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165234.jpg",
      "date_captured": "2023-12-07T22:52:56.1086404Z"
    },
    {
      "id": 36,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164919.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164919.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164919.jpg",
      "date_captured": "2023-12-07T22:52:56.1086486Z"
    },
    {
      "id": 37,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165147.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165147.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165147.jpg",
      "date_captured": "2023-12-07T22:52:56.1086969Z"
    },
    {
      "id": 38,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165202.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165202.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165202.jpg",
      "date_captured": "2023-12-07T22:52:56.1086813Z"
    },
    {
      "id": 39,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165223.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165223.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165223.jpg",
      "date_captured": "2023-12-07T22:52:56.1087088Z"
    },
    {
      "id": 40,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165046.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165046.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165046.jpg",
      "date_captured": "2023-12-07T22:52:56.1086724Z"
    },
    {
      "id": 41,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165020.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165020.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165020.jpg",
      "date_captured": "2023-12-07T22:52:56.1086794Z"
    },
    {
      "id": 42,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165033.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165033.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165033.jpg",
      "date_captured": "2023-12-07T22:52:56.1086746Z"
    },
    {
      "id": 43,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165132.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165132.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165132.jpg",
      "date_captured": "2023-12-07T22:52:56.1087227Z"
    },
    {
      "id": 44,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_165008.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_165008.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_165008.jpg",
      "date_captured": "2023-12-07T22:52:56.1086582Z"
    },
    {
      "id": 45,
      "width": 1024,
      "height": 768,
      "file_name": "IMG_20200229_164851.jpg",
      "coco_url": "AmlDatastore://fruit/IMG_20200229_164851.jpg",
      "absolute_url": "https://<storageAccount>.blob.core.windows.net/fruit/IMG_20200229_164851.jpg",
      "date_captured": "2023-12-07T22:52:56.1086301Z"
    }
  ],
  "annotations": [
    {
      "id": 1,
      "category_id": 1,
      "image_id": 1,
      "area": 0.0
    },
    {
      "id": 2,
      "category_id": 1,
      "image_id": 2,
      "area": 0.0
    },
    {
      "id": 3,
      "category_id": 3,
      "image_id": 3,
      "area": 0.0
    },
    {
      "id": 4,
      "category_id": 2,
      "image_id": 4,
      "area": 0.0
    },
    {
      "id": 5,
      "category_id": 1,
      "image_id": 5,
      "area": 0.0
    },
    {
      "id": 6,
      "category_id": 1,
      "image_id": 6,
      "area": 0.0
    },
    {
      "id": 7,
      "category_id": 1,
      "image_id": 7,
      "area": 0.0
    },
    {
      "id": 8,
      "category_id": 2,
      "image_id": 8,
      "area": 0.0
    },
    {
      "id": 9,
      "category_id": 1,
      "image_id": 9,
      "area": 0.0
    },
    {
      "id": 10,
      "category_id": 2,
      "image_id": 10,
      "area": 0.0
    },
    {
      "id": 11,
      "category_id": 2,
      "image_id": 11,
      "area": 0.0
    },
    {
      "id": 12,
      "category_id": 2,
      "image_id": 12,
      "area": 0.0
    },
    {
      "id": 13,
      "category_id": 1,
      "image_id": 13,
      "area": 0.0
    },
    {
      "id": 14,
      "category_id": 2,
      "image_id": 14,
      "area": 0.0
    },
    {
      "id": 15,
      "category_id": 2,
      "image_id": 15,
      "area": 0.0
    },
    {
      "id": 16,
      "category_id": 1,
      "image_id": 16,
      "area": 0.0
    },
    {
      "id": 17,
      "category_id": 2,
      "image_id": 17,
      "area": 0.0
    },
    {
      "id": 18,
      "category_id": 3,
      "image_id": 18,
      "area": 0.0
    },
    {
      "id": 19,
      "category_id": 3,
      "image_id": 19,
      "area": 0.0
    },
    {
      "id": 20,
      "category_id": 3,
      "image_id": 20,
      "area": 0.0
    },
    {
      "id": 21,
      "category_id": 3,
      "image_id": 21,
      "area": 0.0
    },
    {
      "id": 22,
      "category_id": 3,
      "image_id": 22,
      "area": 0.0
    },
    {
      "id": 23,
      "category_id": 3,
      "image_id": 23,
      "area": 0.0
    },
    {
      "id": 24,
      "category_id": 3,
      "image_id": 24,
      "area": 0.0
    },
    {
      "id": 25,
      "category_id": 1,
      "image_id": 25,
      "area": 0.0
    },
    {
      "id": 26,
      "category_id": 2,
      "image_id": 26,
      "area": 0.0
    },
    {
      "id": 27,
      "category_id": 1,
      "image_id": 27,
      "area": 0.0
    },
    {
      "id": 28,
      "category_id": 3,
      "image_id": 28,
      "area": 0.0
    },
    {
      "id": 29,
      "category_id": 1,
      "image_id": 29,
      "area": 0.0
    },
    {
      "id": 30,
      "category_id": 3,
      "image_id": 30,
      "area": 0.0
    },
    {
      "id": 31,
      "category_id": 3,
      "image_id": 31,
      "area": 0.0
    },
    {
      "id": 32,
      "category_id": 1,
      "image_id": 32,
      "area": 0.0
    },
    {
      "id": 33,
      "category_id": 1,
      "image_id": 33,
      "area": 0.0
    },
    {
      "id": 34,
      "category_id": 2,
      "image_id": 34,
      "area": 0.0
    },
    {
      "id": 35,
      "category_id": 2,
      "image_id": 35,
      "area": 0.0
    },
    {
      "id": 36,
      "category_id": 1,
      "image_id": 36,
      "area": 0.0
    },
    {
      "id": 37,
      "category_id": 2,
      "image_id": 37,
      "area": 0.0
    },
    {
      "id": 38,
      "category_id": 2,
      "image_id": 38,
      "area": 0.0
    },
    {
      "id": 39,
      "category_id": 2,
      "image_id": 39,
      "area": 0.0
    },
    {
      "id": 40,
      "category_id": 3,
      "image_id": 40,
      "area": 0.0
    },
    {
      "id": 41,
      "category_id": 3,
      "image_id": 41,
      "area": 0.0
    },
    {
      "id": 42,
      "category_id": 3,
      "image_id": 42,
      "area": 0.0
    },
    {
      "id": 43,
      "category_id": 2,
      "image_id": 43,
      "area": 0.0
    },
    {
      "id": 44,
      "category_id": 3,
      "image_id": 44,
      "area": 0.0
    },
    {
      "id": 45,
      "category_id": 1,
      "image_id": 45,
      "area": 0.0
    }
  ],
  "categories": [
    {
      "id": 1,
      "name": "apple"
    },
    {
      "id": 2,
      "name": "orange"
    },
    {
      "id": 3,
      "name": "banana"
    }
  ]
}
```

Subscription: Your Azure subscription
Resource group: Choose or create a resource group (if you are using a restricted subscription, you may not have permission to create a new resource group - use the one provided)
Region: Choose from East US, West Europe, West US 2*
Name: Enter a unique name
Pricing tier: Standard S0
*Azure AI Vision 4.0 custom model tags are currently only available in these regions.

Select the required checkboxes and create the resource.

We also need a storage account to store the training images.

In Azure portal, search for and select Storage accounts, and create a new storage account with the following settings:

Subscription: Your Azure subscription
Resource Group: Choose the same resource group you created your Azure AI Service resource in
Storage Account Name: customclassifySUFFIX
note: replace the SUFFIX token with your initials or another value to ensure the resource name is globally unique.
Region: Choose the same region you used for your Azure AI Service resource
Primary service: Azure Blob Storage or Azure Data Lake Storage Gen 2
Primary workload: Other
Performance: Standard
Redundancy: Locally-redundant storage (LRS)
While your storage account is being created, go to Visual studio code, and expand the Labfiles/02-image-classification folder.

In that folder, select replace.ps1 and review the code. You'll see that it replaces the name of your storage account for the placeholder in a JSON file (the COCO file) we use in a later step. Replace the placeholder in the first line only of the file with the name of your storage account. Save the file.

Right-click on the 02-image-classification folder and open an Integrated Terminal. Run the following command.

Your storage account should be complete. Go to your storage account.

Enable public access on the storage account. In the left pane, navigate to Configuration in the Settings group, and enable Allow Blob anonymous access. Select Save

In the left pane, in Data storage, select Containers and create a new container named fruit, and set Anonymous access level to Container (anonymous read access for containers and blobs).

Note: If the Anonymous access level is disabled, refresh the browser page.

Navigate to fruit, select Upload, and upload the images (and the one JSON file) in Labfiles/02-image-classification/training-images to that container.

Create a custom model training project
Next, you will create a new training project for custom image classification in Vision Studio.

In the web browser, navigate to https://portal.vision.cognitive.azure.com/ and sign in with the Microsoft account where you created your Azure AI resource.
Select the Customize models with images tile (can be found in the Image analysis tab if it isn't showing in your default view).
Select the Azure AI Services account you created.
In your project, select Add new dataset on the top. Configure with the following settings:
Dataset name: training_images
Model type: Image classification
Select Azure blob storage container: Select Select Container
Subscription: Your Azure subscription
Storage account: The storage account you created
Blob container: fruit
Select the box to "Allow Vision Studio to read and write to your blob storage"
Select the training_images dataset.
At this point in project creation, you would usually select Create Azure ML Data Labeling Project and label your images, which generates a COCO file. You are encouraged to try this if you have time, but for the purposes of this lab we've already labeled the images for you and supplied the resulting COCO file.

Select Add COCO file
In the dropdown, select Import COCO file from a Blob Container
Since you have already connected your container named fruit, Vision Studio searches that for a COCO file. Select training_labels.json from the dropdown, and add the COCO file.
Navigate to Custom models on the left, and select Train a new model. Use the following settings:
Name of model: classifyfruit
Model type: Image classification
Choose training dataset: training_images
Leave the rest as default, and select Train model
Training can take some time - default budget is up to an hour, however for this small dataset it is usually much quicker than that. Select the Refresh button every couple minutes until the status of the job is Succeeded. Select the model.

Here you can view the performance of the training job. Review the precision and accuracy of the trained model.

Test your custom model
Your model has been trained and is ready to test.

On the top of the page for your custom model, select Try it out.
Select the `classifyfruit` model from the dropdown specifying the model you want to use, and browse to the 02-image-classification\test-images folder.
Select each image and view the results. Select the JSON tab in the results box to examine the full JSON response.

### Lab 5(Analyze text):
[AI language Github](https://github.com/MicrosoftLearning/mslearn-ai-language)
Provision an Azure AI Language resource
If you don't already have one in your subscription, you'll need to provision an Azure AI Language service resource in your Azure subscription.

Open the Azure portal at https://portal.azure.com, and sign in using the Microsoft account associated with your Azure subscription.
Select Create a resource.
In the search field, search for Language service. Then, in the results, select Create under Language Service.
Select Continue to create your resource.
Provision the resource using the following settings:
Subscription: Your Azure subscription.
Resource group: Choose or create a resource group.
Region:Choose any available region
Name: Enter a unique name.
Pricing tier: Select F0 (free), or S (standard) if F is not available.
Responsible AI Notice: Agree.
Select Review + create, then select Create to provision the resource.
Wait for deployment to complete, and then go to the deployed resource.
View the Keys and Endpoint page in the Resource Management section. You will need the information on this page later in the exercise.


```console
pip install azure-ai-textanalytics==5.3.0
pip install python-dotenv
```

```dotenv
AI_SERVICE_ENDPOINT="https://azure-ai-language-service-mm-ttt.cognitiveservices.azure.com/"
AI_SERVICE_KEY="1yYrQqBzdromgZgrAN6f9A4ZF2cq22I6BbGXupWs9N7P4HQ2z8TdJQQJ99AKACYeBjFXJ3w3AAAaACOGgAmK"
```

```python
from dotenv import load_dotenv
import os

# Import namespaces
from azure.core.credentials import AzureKeyCredential
from azure.ai.textanalytics import TextAnalyticsClient

def main():
    try:
        # Get Configuration Settings
        load_dotenv()
        ai_endpoint = os.getenv('AI_SERVICE_ENDPOINT')
        ai_key = os.getenv('AI_SERVICE_KEY')

        # Create client using endpoint and key
        credential = AzureKeyCredential(ai_key)
        ai_client = TextAnalyticsClient(endpoint=ai_endpoint, credential=credential)


        # Analyze each text file in the reviews folder
        reviews_folder = 'reviews'
        for file_name in os.listdir(reviews_folder):
            # Read the file contents
            print('
-------------
' + file_name)
            text = open(os.path.join(reviews_folder, file_name), encoding='utf8').read()
            print('
' + text)

            # Get language
            detectedLanguage = ai_client.detect_language(documents=[text])[0]
            print('
Language: {}'.format(detectedLanguage.primary_language.name))


            # Get sentiment
            sentimentAnalysis = ai_client.analyze_sentiment(documents=[text])[0]
            print("
Sentiment: {}".format(sentimentAnalysis.sentiment))


            # Get key phrases
            phrases = ai_client.extract_key_phrases(documents=[text])[0].key_phrases
            if len(phrases) > 0:
                print("
Key Phrases:")
                for phrase in phrases:
                    print('\t{}'.format(phrase))


            # Get entities
            entities = ai_client.recognize_entities(documents=[text])[0].entities
            if len(entities) > 0:
                print("
Entities")
                for entity in entities:
                    print('\t{} ({})'.format(entity.text, entity.category))


            # Get linked entities
            entities = ai_client.recognize_linked_entities(documents=[text])[0].entities
            if len(entities) > 0:
                print("
Links")
                for linked_entity in entities:
                    print('\t{} ({})'.format(linked_entity.name, linked_entity.url))


    except Exception as ex:
        print(ex)


if __name__ == "__main__":
    main()
```

### Lab 6:
### Lab 7 (Create a language understanding model with the Language service): 
Create a language understanding model with the Language service
NOTE The conversational language understanding feature of the Azure AI Language service is currently in preview, and subject to change. In some cases, model training may fail - if this happens, try again.

The Azure AI Language service enables you to define a conversational language understanding model that applications can use to interpret natural language input from users, predict the users intent (what they want to achieve), and identify any entities to which the intent should be applied.

For example, a conversational language model for a clock application might be expected to process input such as:

What is the time in London?

This kind of input is an example of an utterance (something a user might say or type), for which the desired intent is to get the time in a specific location (an entity); in this case, London.

NOTE The task of a conversational language model is to predict the user's intent and identify any entities to which the intent applies. It is not the job of a conversational language model to actually perform the actions required to satisfy the intent. For example, a clock application can use a conversational language model to discern that the user wants to know the time in London; but the client application itself must then implement the logic to determine the correct time and present it to the user.

Provision an Azure AI Language resource
If you don't already have one in your subscription, you'll need to provision an Azure AI Language service resource in your Azure subscription.

Open the Azure portal at https://portal.azure.com, and sign in using the Microsoft account associated with your Azure subscription.
In the search field at the top, search for Azure AI services. Then, in the results, select Create under Language Service.
Select Continue to create your resource.
Provision the resource using the following settings:
Subscription: Your Azure subscription.
Resource group: Choose or create a resource group.
Region: Choose from one of the following regions*
Australia East
Central India
China East 2
East US
East US 2
North Europe
South Central US
Switzerland North
UK South
West Europe
West US 2
West US 3
Name: Enter a unique name.
Pricing tier: Select F0 (free), or S (standard) if F is not available.
Responsible AI Notice: Agree.
Select Review + create, then select Create to provision the resource.
Wait for deployment to complete, and then go to the deployed resource.
View the Keys and Endpoint page. You will need the information on this page later in the exercise.
Create a conversational language understanding project
Now that you have created an authoring resource, you can use it to create a conversational language understanding project.

In a new browser tab, open the Azure AI Language Studio portal at https://language.cognitive.azure.com/ and sign in using the Microsoft account associated with your Azure subscription.

If prompted to choose a Language resource, select the following settings:

Azure Directory: The Azure directory containing your subscription.
Azure subscription: Your Azure subscription.
Resource type: Language.
Language resource: The Azure AI Language resource you created previously.
If you are not prompted to choose a language resource, it may be because you have multiple Language resources in your subscription; in which case:

On the bar at the top of the page, select the Settings (⚙) button.
On the Settings page, view the Resources tab.
Select the language resource you just created, and click Switch resource.
At the top of the page, click Language Studio to return to the Language Studio home page
At the top of the portal, in the Create new menu, select Conversational language understanding.

In the Create a project dialog box, on the Enter basic information page, enter the following details and then select Next:

Name: Clock
Utterances primary language: English
Enable multiple languages in project?: Unselected
Description: Natural language clock
On the Review and finish page, select Create.

Create intents
The first thing we'll do in the new project is to define some intents. The model will ultimately predict which of these intents a user is requesting when submitting a natural language utterance.

Tip: When working on your project, if some tips are displayed, read them and select Got it to dismiss them, or select Skip all.

On the Schema definition page, on the Intents tab, select ＋ Add to add a new intent named GetTime.
Verify that the GetTime intent is listed (along with the default None intent). Then add the following additional intents:
GetDay
GetDate
Label each intent with sample utterances
To help the model predict which intent a user is requesting, you must label each intent with some sample utterances.

In the pane on the left, select the Data Labeling page.
Tip: You can expand the pane with the >> icon to see the page names, and hide it again with the << icon.

Select the new GetTime intent and enter the utterance what is the time?. This adds the utterance as sample input for the intent.

Add the following additional utterances for the GetTime intent:

what's the time?
what time is it?
tell me the time
NOTE To add a new utterance, write the utterance in the textbox next to the intent and then press ENTER.

Select the GetDay intent and add the following utterances as example input for that intent:

what day is it?
what's the day?
what is the day today?
what day of the week is it?
Select the GetDate intent and add the following utterances for it:

what date is it?
what's the date?
what is the date today?
what's today's date?
After you've added utterances for each of your intents, select Save changes.

Train and test the model
Now that you've added some intents, let's train the language model and see if it can correctly predict them from user input.

In the pane on the left, select Training jobs. Then select + Start a training job.

On the Start a training job dialog, select the option to train a new model, name it Clock. Select Standard training mode and the default Data splitting options.

To begin the process of training your model, select Train.

When training is complete (which may take several minutes) the job Status will change to Training succeeded.

Select the Model performance page, and then select the Clock model. Review the overall and per-intent evaluation metrics (precision, recall, and F1 score) and the confusion matrix generated by the evaluation that was performed when training (note that due to the small number of sample utterances, not all intents may be included in the results).

NOTE To learn more about the evaluation metrics, refer to the documentation

Go to the Deploying a model page, then select Add deployment.

On the Add deployment dialog, select Create a new deployment name, and then enter production.

Select the Clock model in the Model field then select Deploy. The deployment may take some time.

When the model has been deployed, select the Testing deployments page, then select the production deployment in the Deployment name field.

Enter the following text in the empty textbox, and then select Run the test:

what's the time now?

Review the result that is returned, noting that it includes the predicted intent (which should be GetTime) and a confidence score that indicates the probability the model calculated for the predicted intent. The JSON tab shows the comparative confidence for each potential intent (the one with the highest confidence score is the predicted intent)

Clear the text box, and then run another test with the following text:

tell me the time

Again, review the predicted intent and confidence score.

Try the following text:

what's the day today?

Hopefully the model predicts the GetDay intent.

Add entities
So far you've defined some simple utterances that map to intents. Most real applications include more complex utterances from which specific data entities must be extracted to get more context for the intent.

Add a learned entity
The most common kind of entity is a learned entity, in which the model learns to identify entity values based on examples.

In Language Studio, return to the Schema definition page and then on the Entities tab, select ＋ Add to add a new entity.

In the Add an entity dialog box, enter the entity name Location and ensure that the Learned tab is selected. Then select Add entity.

After the Location entity has been created, return to the Data labeling page.

Select the GetTime intent and enter the following new example utterance:

what time is it in London?

When the utterance has been added, select the word London, and in the drop-down list that appears, select Location to indicate that "London" is an example of a location.

Add another example utterance for the GetTime intent:

Tell me the time in Paris?

When the utterance has been added, select the word Paris, and map it to the Location entity.

Add another example utterance for the GetTime intent:

what's the time in New York?

When the utterance has been added, select the words New York, and map them to the Location entity.

Select Save changes to save the new utterances.

Add a list entity
In some cases, valid values for an entity can be restricted to a list of specific terms and synonyms; which can help the app identify instances of the entity in utterances.

In Language Studio, return to the Schema definition page and then on the Entities tab, select ＋ Add to add a new entity.

In the Add an entity dialog box, enter the entity name Weekday and select the List entity tab. Then select Add entity.

On the page for the Weekday entity, in the Learned section, ensure Not required is selected. Then, in the List section, select ＋ Add new list. Then enter the following value and synonym and select Save:

List key	synonyms
Sunday	Sun
NOTE To enter the fields of the new list, insert the value Sunday in the text field, then click on the field where 'Type in value and press enter…' is displayed, enter the synonyms, and press ENTER.

Repeat the previous step to add the following list components:

Value	synonyms
Monday	Mon
Tuesday	Tue, Tues
Wednesday	Wed, Weds
Thursday	Thur, Thurs
Friday	Fri
Saturday	Sat
After adding and saving the list values, return to the Data labeling page.

Select the GetDate intent and enter the following new example utterance:

what date was it on Saturday?

When the utterance has been added, select the word Saturday, and in the drop-down list that appears, select Weekday.

Add another example utterance for the GetDate intent:

what date will it be on Friday?

When the utterance has been added, map Friday to the Weekday entity.

Add another example utterance for the GetDate intent:

what will the date be on Thurs?

When the utterance has been added, map Thurs to the Weekday entity.

select Save changes to save the new utterances.

Add a prebuilt entity
The Azure AI Language service provides a set of prebuilt entities that are commonly used in conversational applications.

In Language Studio, return to the Schema definition page and then on the Entities tab, select ＋ Add to add a new entity.

In the Add an entity dialog box, enter the entity name Date and select the Prebuilt entity tab. Then select Add entity.

On the page for the Date entity, in the Learned section, ensure Not required is selected. Then, in the Prebuilt section, select ＋ Add new prebuilt.

In the Select prebuilt list, select DateTime and then select Save.

After adding the prebuilt entity, return to the Data labeling page

Select the GetDay intent and enter the following new example utterance:

what day was 01/01/1901?

When the utterance has been added, select 01/01/1901, and in the drop-down list that appears, select Date.

Add another example utterance for the GetDay intent:

what day will it be on Dec 31st 2099?

When the utterance has been added, map Dec 31st 2099 to the Date entity.

Select Save changes to save the new utterances.

Retrain the model
Now that you've modified the schema, you need to retrain and retest the model.

On the Training jobs page, select Start a training job.

On the Start a training job dialog, select overwrite an existing model and specify the Clock model. Select Train to train the model. If prompted, confirm you want to overwrite the existing model.

When training is complete the job Status will update to Training succeeded.

Select the Model performance page and then select the Clock model. Review the evaluation metrics (precision, recall, and F1 score) and the confusion matrix generated by the evaluation that was performed when training (note that due to the small number of sample utterances, not all intents may be included in the results).

On the Deploying a model page, select Add deployment.

On the Add deployment dialog, select Override an existing deployment name, and then select production.

Select the Clock model in the Model field and then select Deploy to deploy it. This may take some time.

When the model is deployed, on the Testing deployments page, select the production deployment under the Deployment name field, and then test it with the following text:

- what's the time in Edinburgh?

Review the result that is returned, which should hopefully predict the GetTime intent and a Location entity with the text value "Edinburgh".

Try testing the following utterances:

- what time is it in Tokyo?
- what date is it on Friday?
- what's the date on Weds?
- what day was 01/01/2020?
- what day will Mar 7th 2030 be?

```bash
pip install azure-ai-language-conversations
```
- .env
```dotenv 
LS_CONVERSATIONS_ENDPOINT="https://azure-ai-language-mm-ret.cognitiveservices.azure.com/"
LS_CONVERSATIONS_KEY="7M6pb3ZJPrGRoGkkjqdjjVCsJCuWlszWgVOmHlw1XQHnKPOjIkvtJQQJ99AKACYeBjFXJ3w3AAAaACOGoZam"
```
- clock-client.py
```python 
from dotenv import load_dotenv
import os
import json
from datetime import datetime, timedelta, date, timezone
from dateutil.parser import parse as is_date

# Import namespaces
from azure.core.credentials import AzureKeyCredential
from azure.ai.language.conversations import ConversationAnalysisClient

def main():

    try:
        # Get Configuration Settings
        load_dotenv()
        ls_prediction_endpoint = os.getenv('LS_CONVERSATIONS_ENDPOINT')
        ls_prediction_key = os.getenv('LS_CONVERSATIONS_KEY')

        # Get user input (until they enter "quit")
        userText = ''
        while userText.lower() != 'quit':
            userText = input('
Enter some text ("quit" to stop)
')
            if userText.lower() != 'quit':

                # Create a client for the Language service model
                client = ConversationAnalysisClient(
                    ls_prediction_endpoint, AzureKeyCredential(ls_prediction_key))
                # Call the Language service model to get intent and entities
                cls_project = 'Clock'
                deployment_slot = 'production'

                with client:
                    query = userText
                    result = client.analyze_conversation(
                        task={
                            "kind": "Conversation",
                            "analysisInput": {
                                "conversationItem": {
                                    "participantId": "1",
                                    "id": "1",
                                    "modality": "text",
                                    "language": "en",
                                    "text": query
                                },
                                "isLoggingEnabled": False
                            },
                            "parameters": {
                                "projectName": cls_project,
                                "deploymentName": deployment_slot,
                                "verbose": True
                            }
                        }
                    )

                top_intent = result["result"]["prediction"]["topIntent"]
                entities = result["result"]["prediction"]["entities"]

                print("view top intent:")
                print("\ttop intent: {}".format(result["result"]["prediction"]["topIntent"]))
                print("\tcategory: {}".format(result["result"]["prediction"]["intents"][0]["category"]))
                print("\tconfidence score: {}
".format(result["result"]["prediction"]["intents"][0]["confidenceScore"]))

                print("view entities:")
                for entity in entities:
                    print("\tcategory: {}".format(entity["category"]))
                    print("\ttext: {}".format(entity["text"]))
                    print("\tconfidence score: {}".format(entity["confidenceScore"]))

                print("query: {}".format(result["result"]["query"]))

                # Apply the appropriate action
                if top_intent == 'GetTime':
                    location = 'local'
                    # Check for entities
                    if len(entities) > 0:
                        # Check for a location entity
                        for entity in entities:
                            if 'Location' == entity["category"]:
                                # ML entities are strings, get the first one
                                location = entity["text"]
                    # Get the time for the specified location
                    print(GetTime(location))

                elif top_intent == 'GetDay':
                    date_string = date.today().strftime("%m/%d/%Y")
                    # Check for entities
                    if len(entities) > 0:
                        # Check for a Date entity
                        for entity in entities:
                            if 'Date' == entity["category"]:
                                # Regex entities are strings, get the first one
                                date_string = entity["text"]
                    # Get the day for the specified date
                    print(GetDay(date_string))

                elif top_intent == 'GetDate':
                    day = 'today'
                    # Check for entities
                    if len(entities) > 0:
                        # Check for a Weekday entity
                        for entity in entities:
                            if 'Weekday' == entity["category"]:
                            # List entities are lists
                                day = entity["text"]
                    # Get the date for the specified day
                    print(GetDate(day))

                else:
                    # Some other intent (for example, "None") was predicted
                    print('Try asking me for the time, the day, or the date.')

    except Exception as ex:
        print(ex)


def GetTime(location):
    time_string = ''

    # Note: To keep things simple, we'll ignore daylight savings time and support only a few cities.
    # In a real app, you'd likely use a web service API (or write  more complex code!)
    # Hopefully this simplified example is enough to get the the idea that you
    # use LU to determine the intent and entities, then implement the appropriate logic

    if location.lower() == 'local':
        now = datetime.now()
        time_string = '{}:{:02d}'.format(now.hour,now.minute)
    elif location.lower() == 'london':
        utc = datetime.now(timezone.utc)
        time_string = '{}:{:02d}'.format(utc.hour,utc.minute)
    elif location.lower() == 'sydney':
        time = datetime.now(timezone.utc) + timedelta(hours=11)
        time_string = '{}:{:02d}'.format(time.hour,time.minute)
    elif location.lower() == 'new york':
        time = datetime.now(timezone.utc) + timedelta(hours=-5)
        time_string = '{}:{:02d}'.format(time.hour,time.minute)
    elif location.lower() == 'nairobi':
        time = datetime.now(timezone.utc) + timedelta(hours=3)
        time_string = '{}:{:02d}'.format(time.hour,time.minute)
    elif location.lower() == 'tokyo':
        time = datetime.now(timezone.utc) + timedelta(hours=9)
        time_string = '{}:{:02d}'.format(time.hour,time.minute)
    elif location.lower() == 'delhi':
        time = datetime.now(timezone.utc) + timedelta(hours=5.5)
        time_string = '{}:{:02d}'.format(time.hour,time.minute)
    else:
        time_string = "I don't know what time it is in {}".format(location)
    
    return time_string

def GetDate(day):
    date_string = 'I can only determine dates for today or named days of the week.'

    weekdays = {
        "monday":0,
        "tuesday":1,
        "wednesday":2,
        "thursday":3,
        "friday":4,
        "saturday":5,
        "sunday":6
    }

    today = date.today()

    # To keep things simple, assume the named day is in the current week (Sunday to Saturday)
    day = day.lower()
    if day == 'today':
        date_string = today.strftime("%m/%d/%Y")
    elif day in weekdays:
        todayNum = today.weekday()
        weekDayNum = weekdays[day]
        offset = weekDayNum - todayNum
        date_string = (today + timedelta(days=offset)).strftime("%m/%d/%Y")

    return date_string

def GetDay(date_string):
    # Note: To keep things simple, dates must be entered in US format (MM/DD/YYYY)
    try:
        date_object = datetime.strptime(date_string, "%m/%d/%Y")
        day_string = date_object.strftime("%A")
    except:
        day_string = 'Enter a date in MM/DD/YYYY format.'
    return day_string

if __name__ == "__main__":
    main()

```
- Clock.json
```json 
{
    "api-version": "2021-11-01-preview",
    "metadata": {
        "name": "Clock",
        "description": "Natural language clock",
        "type": "Conversation",
        "multilingual": false,
        "language": "en-us",
        "settings": {
            "confidenceThreshold": 0
        }
    },
    "assets": {
        "intents": [
            {
                "name": "None"
            },
            {
                "name": "GetTime"
            },
            {
                "name": "GetDay"
            },
            {
                "name": "GetDate"
            }
        ],
        "entities": [
            {
                "name": "Location",
                "compositionSetting": "ReturnLongestOverlap",
                "list": null,
                "prebuiltEntities": null
            },
            {
                "name": "Weekday",
                "compositionSetting": "ReturnLongestOverlap",
                "list": {
                    "sublists": [
                        {
                            "listKey": "Saturday",
                            "synonyms": [
                                {
                                    "language": "en-us",
                                    "values": [
                                        "Sat"
                                    ]
                                }
                            ]
                        },
                        {
                            "listKey": "Friday",
                            "synonyms": [
                                {
                                    "language": "en-us",
                                    "values": [
                                        "Fri"
                                    ]
                                }
                            ]
                        },
                        {
                            "listKey": "Thursday",
                            "synonyms": [
                                {
                                    "language": "en-us",
                                    "values": [
                                        "Thu",
                                        "Thur",
                                        "Thurs"
                                    ]
                                }
                            ]
                        },
                        {
                            "listKey": "Wednesday",
                            "synonyms": [
                                {
                                    "language": "en-us",
                                    "values": [
                                        "Wed",
                                        "Weds"
                                    ]
                                }
                            ]
                        },
                        {
                            "listKey": "Tuesday",
                            "synonyms": [
                                {
                                    "language": "en-us",
                                    "values": [
                                        "Tue",
                                        "Tues"
                                    ]
                                }
                            ]
                        },
                        {
                            "listKey": "Monday",
                            "synonyms": [
                                {
                                    "language": "en-us",
                                    "values": [
                                        "Mon"
                                    ]
                                }
                            ]
                        },
                        {
                            "listKey": "Sunday",
                            "synonyms": [
                                {
                                    "language": "en-us",
                                    "values": [
                                        "Sun"
                                    ]
                                }
                            ]
                        }
                    ]
                },
                "prebuiltEntities": null
            },
            {
                "name": "Date",
                "compositionSetting": "ReturnLongestOverlap",
                "list": null,
                "prebuiltEntities": [
                    {
                        "displayName": "DateTime",
                        "semanticType": "DateTime",
                        "semanticSubtype": null
                    }
                ]
            }
        ],
        "examples": [
            {
                "text": "what day will it be on Dec 31st 2099?",
                "language": "en-us",
                "intent": "GetDay",
                "entities": [
                    {
                        "entityName": "Date",
                        "offset": 23,
                        "length": 13
                    }
                ],
                "dataset": "Train"
            },
            {
                "text": "what day was 01/01/1901?",
                "language": "en-us",
                "intent": "GetDay",
                "entities": [
                    {
                        "entityName": "Date",
                        "offset": 13,
                        "length": 10
                    }
                ],
                "dataset": "Train"
            },
            {
                "text": "what will the date be on Thurs?",
                "language": "en-us",
                "intent": "GetDate",
                "entities": [
                    {
                        "entityName": "Weekday",
                        "offset": 25,
                        "length": 5
                    }
                ],
                "dataset": "Train"
            },
            {
                "text": "what date will it be on Friday?",
                "language": "en-us",
                "intent": "GetDate",
                "entities": [
                    {
                        "entityName": "Weekday",
                        "offset": 24,
                        "length": 6
                    }
                ],
                "dataset": "Train"
            },
            {
                "text": "what date was it on Saturday?",
                "language": "en-us",
                "intent": "GetDate",
                "entities": [
                    {
                        "entityName": "Weekday",
                        "offset": 20,
                        "length": 8
                    }
                ],
                "dataset": "Train"
            },
            {
                "text": "what's the time in New York?",
                "language": "en-us",
                "intent": "GetTime",
                "entities": [
                    {
                        "entityName": "Location",
                        "offset": 19,
                        "length": 8
                    }
                ],
                "dataset": "Train"
            },
            {
                "text": "tell me the time in Paris?",
                "language": "en-us",
                "intent": "GetTime",
                "entities": [
                    {
                        "entityName": "Location",
                        "offset": 20,
                        "length": 5
                    }
                ],
                "dataset": "Train"
            },
            {
                "text": "what time is it in London?",
                "language": "en-us",
                "intent": "GetTime",
                "entities": [
                    {
                        "entityName": "Location",
                        "offset": 19,
                        "length": 6
                    }
                ],
                "dataset": "Train"
            },
            {
                "text": "what's today's date?",
                "language": "en-us",
                "intent": "GetDate",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what is the date today?",
                "language": "en-us",
                "intent": "GetDate",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what's the date?",
                "language": "en-us",
                "intent": "GetDate",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what date is it?",
                "language": "en-us",
                "intent": "GetDate",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what day of the week is it?",
                "language": "en-us",
                "intent": "GetDay",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what is the day today?",
                "language": "en-us",
                "intent": "GetDay",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what's the day?",
                "language": "en-us",
                "intent": "GetDay",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what day is it?",
                "language": "en-us",
                "intent": "GetDay",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "tell me the time",
                "language": "en-us",
                "intent": "GetTime",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what time is it?",
                "language": "en-us",
                "intent": "GetTime",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what's the time?",
                "language": "en-us",
                "intent": "GetTime",
                "entities": [],
                "dataset": "Train"
            },
            {
                "text": "what is the time?",
                "language": "en-us",
                "intent": "GetTime",
                "entities": [],
                "dataset": "Train"
            }
        ]
    }
}
```
